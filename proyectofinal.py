# -*- coding: utf-8 -*-
"""ProyectoFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vhBlBzAoUAKPVSaz6zoLq9AavNh6mu6U
"""

!pip install spacy

!pip install spacy

!pip install pandas

!pip install nltk

!pip install language-tool-python

!pip install openpyxl

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk import sentiment
from nltk import sent_tokenize

import nltk
import spacy
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from spacy.lang.en.stop_words import STOP_WORDS

datos_excel = pd.read_excel("/content/drive/MyDrive/Colab Notebooks/Respuestas.xlsx", engine="openpyxl")

if 'Marca temporal' in datos_excel.columns:
    datos_excel = datos_excel.drop('Marca temporal', axis=1)
else:
    print("La columna 'Marca temporal' no existe en el DataFrame.")

# Separar las columnas en datos numéricos y categóricas (de texto)
numeric_data = datos_excel.select_dtypes(include=['number'])  # Columnas numéricas
categorical_data = datos_excel.select_dtypes(include=['object'])  # Columnas categóricas con respuestas

print("Columnas numéricas:")
print(numeric_data)

print("\nColumnas categóricas:")
print(categorical_data)

# Validar si existen columnas categóricas
if categorical_data.empty:
    print("No se encontraron columnas categóricas para usar como etiquetas.")
else:
    # Usar la primera columna categórica como 'target_column'
    target_column = categorical_data.columns[0]
    y = datos_excel[target_column]  # Etiquetas
    X = numeric_data  # Datos numéricos como características

    # Imprimir resultados
    print("Columna seleccionada como target (y):")
    print(target_column)
    print("\nDatos categóricos (y):")
    print(y)
    print("\nDatos numéricos (X):")
    print(X)

# Validar si X o Y están vacíos
if X.empty or y.empty:
        print("Faltan datos numéricos o etiquetas categóricas.")
else:
        # Imprimir resultados
        print("Columna seleccionada como target (y):")
        print(target_column)
        print("\nDatos categóricos (y):")
        print(y)
        print("\nDatos numéricos (X):")
        print(X)

!pip install wordcloud
!pip install seaborn

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

if not numeric_data.empty:
    print("Estadísticas descriptivas de las columnas numéricas:")
    print(numeric_data.describe())

    # Visualización de histogramas para cada columna numérica
    print("\nGenerando histogramas de columnas numéricas...")
    for column in numeric_data.columns:
        plt.figure(figsize=(8, 4))
        sns.histplot(numeric_data[column], kde=True)
        plt.title(f'Histograma de {column}')
        plt.xlabel(column)
        plt.ylabel('Frecuencia')
        plt.show()

    # Visualización de diagramas de dispersión para pares de columnas numéricas
    if numeric_data.shape[1] > 1:
        sns.pairplot(numeric_data)
        plt.show()

if not categorical_data.empty:
    print("\nAnálisis básico de columnas categóricas/texto:")

    # Generar nube de palabras para las columnas categóricas
    for column in categorical_data.columns:
        text = " ".join(categorical_data[column].dropna().astype(str))
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

        # Mostrar la nube de palabras
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.title(f'Nube de palabras para {column}')
        plt.axis('off')
        plt.show()
else:
    print("No hay columnas categóricas para analizar.")

import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

numeric_data = datos_excel.select_dtypes(include=['number'])
if not numeric_data.empty:
    scaler = StandardScaler()
    standardized_data = scaler.fit_transform(numeric_data)
    distortions = []
    silhouette_scores = []
    K = range(2, 11)
    for k in K:
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(standardized_data)
        distortions.append(kmeans.inertia_)
        cluster_labels = kmeans.predict(standardized_data)
        silhouette_scores.append(silhouette_score(standardized_data, cluster_labels))

    # Gráfica del método del codo
    plt.figure(figsize=(8, 4))
    plt.plot(K, distortions, marker='o', label='Inercia')
    plt.title('Método del codo')
    plt.xlabel('Número de clústeres (K)')
    plt.ylabel('Inercia')
    plt.grid()
    plt.legend()
    plt.show()

    # Gráfica del silhouette score
    plt.figure(figsize=(8, 4))
    plt.plot(K, silhouette_scores, marker='o', color='green', label='Silhouette Score')
    plt.title('Silhouette Score para diferentes valores de K')
    plt.xlabel('Número de clústeres (K)')
    plt.ylabel('Silhouette Score')
    plt.grid()
    plt.legend()
    plt.show()

    # 3. Aplicar K-means con el número óptimo de clústeres
    optimal_k = 4
    kmeans = KMeans(n_clusters=optimal_k, random_state=42)
    cluster_labels = kmeans.fit_predict(standardized_data)
    datos_excel['Cluster'] = cluster_labels

    # 4. Definir nombres descriptivos para los clústeres
    cluster_analysis = {
        0: "Estudiantes satisfechos con infraestructura",
        1: "Estudiantes que valoran las actividades extracurriculares",
        2: "Grupo con interés en el apoyo estudiantil",
        3: "Estudiantes que necesitan mejorar la experiencia académica",
    }

    # Asignar los nombres descriptivos al DataFrame
    datos_excel['Cluster Descripción'] = datos_excel['Cluster'].map(cluster_analysis)
    print("Resumen de clústeres con descripciones:")
    print(datos_excel[['Cluster', 'Cluster Descripción']].drop_duplicates())

    if standardized_data.shape[1] >= 4:
        plt.figure(figsize=(8, 6))
        plt.scatter(standardized_data[:, 0], standardized_data[:, 1], c=cluster_labels, cmap='viridis', s=50)
        plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=200, alpha=0.75, label='Centroides')
        plt.title('Visualización de los clústeres (Primeras dos dimensiones)')

        plt.legend()
        plt.grid()
        plt.show()

else:
    print("No hay datos numéricos para aplicar K-means.")

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import LabelEncoder

# Filtrar las columnas categóricas
categorical_columns = datos_excel.select_dtypes(include=['object']).columns

# Usar las columnas numéricas como características
X = numeric_data

if not X.empty and not categorical_columns.empty:
    for column in categorical_columns:
        print(f"\nProcesando columna: {column}")
        y = datos_excel[column]

        class_counts = y.value_counts()
        valid_classes = class_counts[class_counts >= 3].index
        y_filtered = y[y.isin(valid_classes)]
        X_filtered = X.loc[y_filtered.index]

        if y_filtered.empty:
            print(f"No hay suficientes datos para procesar la columna '{column}' después del filtrado.")
            continue

        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(y_filtered)

        # Dividir los datos en entrenamiento (80%) y prueba (20%)
        X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_encoded, test_size=0.2, random_state=42)

        # Entrenar un modelo Naive Bayes
        model = GaussianNB()
        model.fit(X_train, y_train)

        # Evaluar el modelo
        y_pred = model.predict(X_test)
        unique_classes = label_encoder.inverse_transform(sorted(set(y_test)))
        print(f"Reporte de clasificación para la columna '{column}':")
        print(classification_report(y_test, y_pred, labels=sorted(set(y_test)), target_names=unique_classes, zero_division=1))

else:
    print("No se encontraron columnas categóricas o numéricas adecuadas para entrenar un modelo.")

y = y.astype('category')  # Convertir a categórico si no lo es

# Crear la matriz de confusión y graficarla
categories = y.cat.categories  # Extraer las categorías
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)
plt.title("Matriz de Confusión")
plt.xlabel("Predicción")
plt.ylabel("Verdad")
plt.show()

# Reporte de clasificación ajustado
print("\nReporte de Clasificación:")
print(classification_report(y_test, y_pred, target_names=y.cat.categories, zero_division=0))